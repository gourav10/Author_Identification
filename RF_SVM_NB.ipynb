{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ht_13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ht_13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/ht_13/OneDrive/Documents/6140-ML/Project/'\n",
    "# load test result file\n",
    "test_author = scipy.io.loadmat(path+'dataset/dataset/test_author.mat')[\"test_author\"]\n",
    "# load train dataFrame\n",
    "train_df = pd.read_csv(path+'dataset/dataset/Gungor_2018_VictorianAuthorAttribution_data-train.csv',encoding = \"ISO-8859-1\")\n",
    "# load train dataFrame\n",
    "test_df = pd.read_csv(path+'dataset/dataset/Gungor_2018_VictorianAuthorAttribution_data.csv',encoding = \"ISO-8859-1\")\n",
    "# load list of authors\n",
    "f = open('author_list.txt', 'r')\n",
    "author_list = f.read().split('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Arthur Conan Doyle\n",
      "2 Charles Darwin\n",
      "3 Charles Dickens\n",
      "4 Edith Wharton\n",
      "5 George Eliot\n",
      "6 Horace Greeley\n",
      "7 Jack London\n",
      "8 James Baldwin\n",
      "9 Jane Austen\n",
      "10 John Muir\n",
      "11 Joseph Conrad\n",
      "12 Mark Twain\n",
      "13 Nathaniel Hawthorne\n",
      "14 Ralph Emerson\n",
      "15 Robert Louis Stevenson\n",
      "16 Rudyard Kipling\n",
      "17 Sinclair Lewis\n",
      "18 Theodore Dreiser\n",
      "19 Thomas Hardy\n",
      "20 Walt Whitman\n",
      "21 Washington Irving\n",
      "22 William Carleton\n",
      "23 Albert Ross\n",
      "24 Anne Manning\n",
      "25 Arlo Bates\n",
      "26 Bret Harte\n",
      "27 Catharine Maria Sedgwick\n",
      "28 Charles Reade\n",
      "29 Edward Eggleston\n",
      "30 Fergus Hume\n",
      "31 Frances Hodgson Burnett\n",
      "32 George Moore\n",
      "33 George William Curtis\n",
      "34 Helen Mathers\n",
      "35 Henry Rider Haggard\n",
      "36 Isabella Lucy Bird\n",
      "37 Jacob Abbott\n",
      "38 James Grant\n",
      "39 James Payn\n",
      "40 John Kendrick Bangs\n",
      "41 John Pendleton Kennedy\n",
      "42 John Strange Winter\n",
      "43 Lucas Malet\n",
      "44 Marie Corelli\n",
      "45 Oliver Optic\n",
      "46 Sarah Orne Jewett\n",
      "47 Sarah Stickney Ellis\n",
      "48 Thomas Anstey Guthrie\n",
      "49 Thomas Nelson Page\n",
      "50 William Black\n"
     ]
    }
   ],
   "source": [
    "# print list of authers and create map dict\n",
    "author_name_map_dict ={}\n",
    "author_id_map_dict ={}\n",
    "for i,author in enumerate(author_list):\n",
    "    author_name_map_dict[author] = i\n",
    "    author_id_map_dict[i] = author\n",
    "    print(i+1, author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class distribution for all the authors\n",
    "cnt_srs = train_df['author'].value_counts()\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Author Name', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample number:  (53678, 2)\n",
      "Test sample number:  (38809, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing number of train and test samples\n",
    "print(\"Training sample number: \", train_df.shape)\n",
    "print(\"Test sample number: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ou have time to listen i will give you the ent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wish for solitude he was twenty years of age a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and the skirt blew in perfect freedom about th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of san and the rows of shops opposite impresse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an hour s walk was as tiresome as three in a s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  author\n",
       "0  ou have time to listen i will give you the ent...       1\n",
       "1  wish for solitude he was twenty years of age a...       1\n",
       "2  and the skirt blew in perfect freedom about th...       1\n",
       "3  of san and the rows of shops opposite impresse...       1\n",
       "4  an hour s walk was as tiresome as three in a s...       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each line consists of 1000 words\n",
    "str_list = train_df[\"text\"][0].split(\" \")\n",
    "str_list = filter(None, str_list) \n",
    "len(list(str_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 53678/53678 [00:52<00:00, 1027.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 38809/38809 [00:37<00:00, 1021.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda words: ' '.join(word.lower() for word in words.split(' ') if word not in stop))\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda words: ' '.join(word.lower() for word in words.split(' ') if word not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def remove_punctuations(text_context):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    new_words = tokenizer.tokenize(text_context)\n",
    "    return ' '.join([word for word in new_words])\n",
    "\n",
    "def stemming_data(text_content):\n",
    "    words = word_tokenize(text_content)\n",
    "    return ' '.join([stemmer.stem(word) for word in words])\n",
    "\n",
    "def lemmatize_data(text_content):\n",
    "    words = word_tokenize(text_content)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda x: remove_punctuations(x))\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda x: remove_punctuations(x))\n",
    "\n",
    "#Perform stemming\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda x: stemming_data(x))\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda x: stemming_data(x))\n",
    "\n",
    "# and lemmatization\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda x: lemmatize_data(x))\n",
    "test_df['text'] = test_df['text'].progress_apply(lambda x: lemmatize_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training data: 42942\n",
      "Length of Validation data : 10736\n"
     ]
    }
   ],
   "source": [
    "# split the data in train and validation set\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_df.text.values, train_df.author.values, \n",
    "                                                  stratify=train_df.author.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "print ('Length of Training data:',xtrain.shape[0])\n",
    "print ('Length of Validation data :',xvalid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in train and validation set\n",
    "df_train, df_val = train_test_split(train_df ,stratify=train_df.author.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "train_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pure test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['author'] = test_author.reshape((test_df.shape[0]))\n",
    "missing_authors = [5, 7, 31, 47, 49]\n",
    "df_test = test_df.loc[~(test_df['author'].isin(missing_authors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest,ytest = df_test.text.values,df_test.author.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 23s\n",
      "Wall time: 39.2 s\n",
      "Wall time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "# embedding using TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=10000, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and validation sets\n",
    "%time tfv.fit(list(xtrain) + list(xvalid))\n",
    "%time xtrain_tfv =  tfv.transform(xtrain)\n",
    "%time xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%time xtest_tfv = tfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 50s\n",
      "Random Forest with TFIDF: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.29      0.44       182\n",
      "           2       1.00      0.05      0.10        76\n",
      "           3       1.00      0.19      0.31        43\n",
      "           4       0.82      0.39      0.53       297\n",
      "           6       1.00      0.19      0.31        81\n",
      "           8       0.42      0.97      0.58      1383\n",
      "           9       0.94      0.42      0.58       222\n",
      "          10       0.99      0.77      0.87       151\n",
      "          11       0.90      0.56      0.69        77\n",
      "          12       0.96      0.18      0.31       125\n",
      "          13       1.00      0.47      0.64        97\n",
      "          14       0.84      0.79      0.81       539\n",
      "          15       0.93      0.63      0.76       292\n",
      "          16       1.00      0.19      0.32        37\n",
      "          17       0.65      0.10      0.17       132\n",
      "          18       0.92      0.43      0.59       216\n",
      "          19       0.82      0.91      0.86       309\n",
      "          20       0.93      0.86      0.89       117\n",
      "          21       0.71      0.87      0.78       461\n",
      "          22       0.97      0.38      0.55        99\n",
      "          23       0.97      0.38      0.55        91\n",
      "          24       1.00      0.42      0.59        76\n",
      "          25       0.90      0.16      0.28       232\n",
      "          26       0.76      0.99      0.86       888\n",
      "          27       0.96      0.38      0.54        61\n",
      "          28       0.87      0.81      0.84       165\n",
      "          29       0.97      0.46      0.62       129\n",
      "          30       0.85      0.58      0.69       194\n",
      "          32       0.88      0.43      0.57       141\n",
      "          33       0.86      0.72      0.78       348\n",
      "          34       0.97      0.68      0.80        91\n",
      "          35       1.00      0.25      0.40       132\n",
      "          36       0.99      0.83      0.90       139\n",
      "          37       0.55      0.92      0.69       477\n",
      "          38       0.95      0.54      0.69       233\n",
      "          39       0.84      0.91      0.88       453\n",
      "          40       1.00      0.19      0.31        86\n",
      "          41       0.89      0.36      0.52       182\n",
      "          42       0.98      0.93      0.96       204\n",
      "          43       0.92      0.69      0.79       253\n",
      "          44       1.00      0.10      0.17        94\n",
      "          45       0.83      0.72      0.77       462\n",
      "          46       1.00      0.35      0.52       121\n",
      "          48       0.78      0.85      0.81       365\n",
      "          50       0.98      0.54      0.70       183\n",
      "\n",
      "    accuracy                           0.69     10736\n",
      "   macro avg       0.90      0.53      0.61     10736\n",
      "weighted avg       0.80      0.69      0.68     10736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "%time clf.fit(xtrain_tfv, ytrain)\n",
    "print(\"Random Forest with TFIDF:\", clf.score(xtrain_tfv, ytrain))\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed (with compilation) = 22.010324001312256\n"
     ]
    }
   ],
   "source": [
    "# embeddings using CountVectorizer\n",
    "def trial_ctv(xtrain, xvalid):\n",
    "    ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}', stop_words = 'english')\n",
    "\n",
    "    # Fitting Count Vectorizer to both training and test sets\n",
    "    ctv.fit(list(xtrain) + list(xvalid))\n",
    "    xtrain_ctv =  ctv.transform(xtrain) \n",
    "    xvalid_ctv = ctv.transform(xvalid)\n",
    "    return xtrain_ctv, xvalid_ctv, ctv\n",
    "\n",
    "start = time.time()\n",
    "xtrain_ctv, xvalid_ctv, ctv = trial_ctv(xtrain, xvalid)\n",
    "end = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%time xtest_ctv = ctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 15s\n",
      "Random Forest with CV: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.02      0.04       182\n",
      "           2       0.00      0.00      0.00        76\n",
      "           3       0.00      0.00      0.00        43\n",
      "           4       0.86      0.17      0.28       297\n",
      "           6       1.00      0.15      0.26        81\n",
      "           8       0.24      0.99      0.38      1383\n",
      "           9       1.00      0.02      0.04       222\n",
      "          10       1.00      0.59      0.74       151\n",
      "          11       1.00      0.22      0.36        77\n",
      "          12       0.00      0.00      0.00       125\n",
      "          13       1.00      0.02      0.04        97\n",
      "          14       0.96      0.55      0.70       539\n",
      "          15       0.98      0.32      0.48       292\n",
      "          16       1.00      0.19      0.32        37\n",
      "          17       0.60      0.02      0.04       132\n",
      "          18       0.97      0.15      0.26       216\n",
      "          19       0.81      0.84      0.82       309\n",
      "          20       1.00      0.45      0.62       117\n",
      "          21       0.89      0.75      0.82       461\n",
      "          22       1.00      0.03      0.06        99\n",
      "          23       0.95      0.22      0.36        91\n",
      "          24       1.00      0.01      0.03        76\n",
      "          25       0.71      0.02      0.04       232\n",
      "          26       0.91      0.99      0.95       888\n",
      "          27       0.92      0.18      0.30        61\n",
      "          28       0.95      0.72      0.82       165\n",
      "          29       1.00      0.09      0.17       129\n",
      "          30       1.00      0.11      0.20       194\n",
      "          32       0.96      0.18      0.31       141\n",
      "          33       0.97      0.48      0.64       348\n",
      "          34       0.94      0.53      0.68        91\n",
      "          35       1.00      0.12      0.22       132\n",
      "          36       1.00      0.36      0.53       139\n",
      "          37       0.48      0.87      0.62       477\n",
      "          38       0.98      0.42      0.59       233\n",
      "          39       0.97      0.83      0.89       453\n",
      "          40       1.00      0.12      0.21        86\n",
      "          41       1.00      0.04      0.08       182\n",
      "          42       1.00      0.79      0.88       204\n",
      "          43       0.99      0.47      0.63       253\n",
      "          44       1.00      0.06      0.12        94\n",
      "          45       0.97      0.39      0.56       462\n",
      "          46       1.00      0.16      0.27       121\n",
      "          48       0.93      0.66      0.77       365\n",
      "          50       1.00      0.08      0.15       183\n",
      "\n",
      "    accuracy                           0.53     10736\n",
      "   macro avg       0.87      0.32      0.38     10736\n",
      "weighted avg       0.81      0.53      0.51     10736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "%time clf.fit(xtrain_ctv, ytrain)\n",
    "print(\"Random Forest with CV:\", clf.score(xtrain_ctv, ytrain))\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47min 13s\n",
      "SVM with linear kernel and TFIDF Vectorizer: 0.9983000326021145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.99      0.99       182\n",
      "           2       0.99      0.95      0.97        76\n",
      "           3       1.00      1.00      1.00        43\n",
      "           4       0.91      0.87      0.89       297\n",
      "           6       1.00      0.98      0.99        81\n",
      "           8       0.97      0.99      0.98      1383\n",
      "           9       1.00      0.99      0.99       222\n",
      "          10       1.00      0.99      0.99       151\n",
      "          11       0.99      0.86      0.92        77\n",
      "          12       1.00      0.98      0.99       125\n",
      "          13       1.00      0.98      0.99        97\n",
      "          14       1.00      1.00      1.00       539\n",
      "          15       1.00      1.00      1.00       292\n",
      "          16       0.97      0.81      0.88        37\n",
      "          17       0.93      0.98      0.96       132\n",
      "          18       0.99      1.00      0.99       216\n",
      "          19       0.99      0.99      0.99       309\n",
      "          20       1.00      1.00      1.00       117\n",
      "          21       1.00      0.98      0.99       461\n",
      "          22       1.00      0.99      0.99        99\n",
      "          23       0.93      0.89      0.91        91\n",
      "          24       1.00      0.96      0.98        76\n",
      "          25       0.98      0.97      0.98       232\n",
      "          26       1.00      1.00      1.00       888\n",
      "          27       0.98      0.97      0.98        61\n",
      "          28       0.98      1.00      0.99       165\n",
      "          29       0.99      0.95      0.97       129\n",
      "          30       0.99      0.99      0.99       194\n",
      "          32       0.97      0.96      0.97       141\n",
      "          33       0.99      0.99      0.99       348\n",
      "          34       0.98      0.93      0.96        91\n",
      "          35       1.00      0.93      0.96       132\n",
      "          36       1.00      0.99      1.00       139\n",
      "          37       0.91      0.99      0.95       477\n",
      "          38       0.98      0.97      0.98       233\n",
      "          39       0.99      0.97      0.98       453\n",
      "          40       1.00      1.00      1.00        86\n",
      "          41       0.97      0.97      0.97       182\n",
      "          42       1.00      1.00      1.00       204\n",
      "          43       0.99      1.00      0.99       253\n",
      "          44       0.99      0.98      0.98        94\n",
      "          45       0.98      0.99      0.99       462\n",
      "          46       1.00      0.97      0.98       121\n",
      "          48       0.96      0.98      0.97       365\n",
      "          50       0.98      0.97      0.98       183\n",
      "\n",
      "    accuracy                           0.98     10736\n",
      "   macro avg       0.98      0.97      0.98     10736\n",
      "weighted avg       0.98      0.98      0.98     10736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = svm.SVC(kernel='linear',verbose=2)\n",
    "%time clf.fit(xtrain_tfv, ytrain)\n",
    "%time print(\"SVM with linear kernel and TFIDF Vectorizer:\", clf.score(xtrain_tfv, ytrain))\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SVM_tfv_linear.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.92      0.92       475\n",
      "           2       0.86      0.26      0.40       125\n",
      "           3       1.00      0.21      0.35       151\n",
      "           4       0.62      0.90      0.74       992\n",
      "           6       0.94      0.55      0.69       214\n",
      "           8       0.80      0.99      0.88      3270\n",
      "           9       0.97      0.82      0.89       503\n",
      "          10       0.97      0.71      0.82       396\n",
      "          11       0.67      0.76      0.71        98\n",
      "          12       0.98      0.47      0.63       355\n",
      "          13       1.00      0.54      0.70       323\n",
      "          14       0.95      0.97      0.96      1228\n",
      "          15       0.94      0.90      0.92       505\n",
      "          16       1.00      0.31      0.48       215\n",
      "          17       0.92      0.83      0.88       384\n",
      "          18       0.89      0.76      0.82       753\n",
      "          19       0.63      0.76      0.69       522\n",
      "          20       0.99      0.89      0.94       248\n",
      "          21       0.92      0.99      0.95      1086\n",
      "          22       0.98      0.61      0.75       193\n",
      "          23       0.82      0.63      0.71       381\n",
      "          24       0.54      0.18      0.27       444\n",
      "          25       0.90      0.90      0.90       364\n",
      "          26       0.99      1.00      1.00      1934\n",
      "          27       1.00      0.42      0.59       137\n",
      "          28       0.91      0.98      0.94       338\n",
      "          29       0.93      0.91      0.92       158\n",
      "          30       0.81      0.89      0.85       310\n",
      "          32       0.95      0.75      0.84       467\n",
      "          33       0.86      0.74      0.79       429\n",
      "          34       0.86      0.23      0.37       344\n",
      "          35       0.88      0.51      0.64       257\n",
      "          36       1.00      0.71      0.83       469\n",
      "          37       0.50      0.94      0.66       913\n",
      "          38       0.95      0.86      0.90       443\n",
      "          39       0.93      0.96      0.94      1374\n",
      "          40       0.99      0.95      0.97       229\n",
      "          41       0.79      0.80      0.79       334\n",
      "          42       0.98      0.92      0.95       328\n",
      "          43       0.95      0.94      0.95       404\n",
      "          44       0.94      0.88      0.91       321\n",
      "          45       0.86      0.96      0.90      1156\n",
      "          46       0.99      0.82      0.90       490\n",
      "          48       0.90      0.96      0.93       902\n",
      "          50       0.96      0.92      0.94       674\n",
      "\n",
      "    accuracy                           0.85     25636\n",
      "   macro avg       0.89      0.75      0.79     25636\n",
      "weighted avg       0.87      0.85      0.85     25636\n",
      "\n",
      "Wall time: 53.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time test_predictions = clf.predict(xtest_tfv)\n",
    "%time print(classification_report(ytest, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 8s\n",
      "Wall time: 10min 30s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.98      0.98       182\n",
      "           2       0.96      0.88      0.92        76\n",
      "           3       1.00      0.95      0.98        43\n",
      "           4       0.88      0.85      0.86       297\n",
      "           6       0.99      0.89      0.94        81\n",
      "           8       0.96      0.99      0.97      1383\n",
      "           9       0.99      0.96      0.98       222\n",
      "          10       0.99      0.96      0.98       151\n",
      "          11       0.97      0.83      0.90        77\n",
      "          12       0.98      0.98      0.98       125\n",
      "          13       0.98      0.97      0.97        97\n",
      "          14       0.99      0.99      0.99       539\n",
      "          15       0.98      0.98      0.98       292\n",
      "          16       0.97      0.81      0.88        37\n",
      "          17       0.93      0.96      0.94       132\n",
      "          18       0.97      0.99      0.98       216\n",
      "          19       0.96      0.97      0.97       309\n",
      "          20       1.00      0.99      1.00       117\n",
      "          21       0.98      0.98      0.98       461\n",
      "          22       1.00      0.95      0.97        99\n",
      "          23       0.91      0.85      0.87        91\n",
      "          24       0.99      0.95      0.97        76\n",
      "          25       0.98      0.96      0.97       232\n",
      "          26       1.00      1.00      1.00       888\n",
      "          27       0.97      0.92      0.94        61\n",
      "          28       0.95      0.98      0.96       165\n",
      "          29       0.99      0.95      0.97       129\n",
      "          30       0.97      0.99      0.98       194\n",
      "          32       0.97      0.97      0.97       141\n",
      "          33       0.97      0.97      0.97       348\n",
      "          34       0.97      0.91      0.94        91\n",
      "          35       0.97      0.89      0.92       132\n",
      "          36       1.00      0.98      0.99       139\n",
      "          37       0.92      0.97      0.95       477\n",
      "          38       0.97      0.97      0.97       233\n",
      "          39       0.98      0.96      0.97       453\n",
      "          40       0.99      1.00      0.99        86\n",
      "          41       0.96      0.91      0.94       182\n",
      "          42       1.00      1.00      1.00       204\n",
      "          43       0.99      0.99      0.99       253\n",
      "          44       0.94      0.95      0.94        94\n",
      "          45       0.96      0.98      0.97       462\n",
      "          46       1.00      0.95      0.97       121\n",
      "          48       0.95      0.99      0.97       365\n",
      "          50       0.96      0.93      0.95       183\n",
      "\n",
      "    accuracy                           0.97     10736\n",
      "   macro avg       0.97      0.95      0.96     10736\n",
      "weighted avg       0.97      0.97      0.97     10736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "%time clf.fit(xtrain_ctv, ytrain)\n",
    "# print(\"SVM with linear kernel and Count Vectorizer:\", clf.score(xtrain_ctv, ytrain))\n",
    "%time predictions = clf.predict(xvalid_ctv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SVM_ctv_linear.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25min 8s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.89      0.87       475\n",
      "           2       0.53      0.30      0.38       125\n",
      "           3       0.90      0.35      0.50       151\n",
      "           4       0.55      0.77      0.65       992\n",
      "           6       0.77      0.42      0.54       214\n",
      "           8       0.79      0.97      0.87      3270\n",
      "           9       0.90      0.70      0.79       503\n",
      "          10       0.94      0.69      0.80       396\n",
      "          11       0.52      0.72      0.60        98\n",
      "          12       0.88      0.32      0.47       355\n",
      "          13       0.92      0.50      0.65       323\n",
      "          14       0.89      0.94      0.92      1228\n",
      "          15       0.87      0.78      0.82       505\n",
      "          16       1.00      0.30      0.46       215\n",
      "          17       0.80      0.73      0.76       384\n",
      "          18       0.73      0.70      0.72       753\n",
      "          19       0.64      0.75      0.69       522\n",
      "          20       0.90      0.86      0.88       248\n",
      "          21       0.90      0.96      0.93      1086\n",
      "          22       0.93      0.51      0.66       193\n",
      "          23       0.74      0.56      0.63       381\n",
      "          24       0.56      0.15      0.24       444\n",
      "          25       0.73      0.85      0.79       364\n",
      "          26       0.98      1.00      0.99      1934\n",
      "          27       0.88      0.27      0.41       137\n",
      "          28       0.86      0.98      0.91       338\n",
      "          29       0.76      0.88      0.82       158\n",
      "          30       0.73      0.77      0.75       310\n",
      "          32       0.93      0.75      0.83       467\n",
      "          33       0.68      0.65      0.66       429\n",
      "          34       0.80      0.22      0.35       344\n",
      "          35       0.75      0.44      0.56       257\n",
      "          36       0.97      0.66      0.79       469\n",
      "          37       0.55      0.92      0.69       913\n",
      "          38       0.88      0.84      0.86       443\n",
      "          39       0.93      0.94      0.93      1374\n",
      "          40       0.96      0.93      0.94       229\n",
      "          41       0.70      0.78      0.74       334\n",
      "          42       0.95      0.91      0.93       328\n",
      "          43       0.90      0.95      0.92       404\n",
      "          44       0.87      0.81      0.84       321\n",
      "          45       0.81      0.87      0.84      1156\n",
      "          46       0.95      0.77      0.85       490\n",
      "          48       0.83      0.93      0.88       902\n",
      "          50       0.94      0.89      0.91       674\n",
      "\n",
      "    accuracy                           0.81     25636\n",
      "   macro avg       0.82      0.71      0.73     25636\n",
      "weighted avg       0.83      0.81      0.80     25636\n",
      "\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time test_predictions = clf.predict(xtest_ctv)\n",
    "%time print(classification_report(ytest, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='RBF', gamma=0.7, C = 1,verbose=2)\n",
    "%time clf.fit(xtrain_tfv, ytrain)\n",
    "print(\"SVM with RBF kernel and TFIDF Vectorizer:\", clf.score(xtrain_tfv, ytrain))\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='RBF', gamma=0.7, C = 1,verbose=2)\n",
    "%time clf.fit(xtrain_ctv, ytrain)\n",
    "# print(\"SVM with RBF kernel and Count Vectorizer:\", clf.score(xtrain_ctv, ytrain))\n",
    "%time predictions = clf.predict(xvalid_ctv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='Polynomial', gamma=0.7, C = 1,verbose=2)\n",
    "%time clf.fit(xtrain_tfv, ytrain)\n",
    "print(\"SVM with Polynomial kernel and TFIDF Vectorizer:\", clf.score(xtrain_tfv, ytrain))\n",
    "predictions = clf.predict(xvalid_tfv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='Polynomial', gamma=0.7, C = 1,verbose=2)\n",
    "%time clf.fit(xtrain_ctv, ytrain)\n",
    "print(\"SVM with Polynomial kernel and Count Vectorizer:\", clf.score(xtrain_ctv, ytrain))\n",
    "predictions = clf.predict(xvalid_ctv)\n",
    "print(classification_report(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain) #Train Naive Bayes classifier\n",
    "yv = clf.predict(xvalid_ctv) #validation set\n",
    "print(classification_report(yvalid, yv)) #precision, recall and f1 score on the validation set\n",
    "xtest = test_df.text.values\n",
    "yt = test_author#test set\n",
    "# Fitting Count Vectorizer to both test set\n",
    "xtest_ctv =  ctv.transform(xtest) \n",
    "ytest = clf.predict(xtest_ctv) #test set\n",
    "print(classification_report(yt, ytest)) #precision, recall and f1 score on t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
